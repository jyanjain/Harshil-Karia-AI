# -*- coding: utf-8 -*-
"""harshil_karia_backend.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/138Uk-4oCLFHjHk6xEAqV2CzENrlxAkJL
"""

!pip install fastapi uvicorn torch transformers accelerate pyngrok

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import logging
import uvicorn
import threading
from pyngrok import ngrok
import time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Request model
class ChatRequest(BaseModel):
    message: str
    max_new_tokens: int = 200
    temperature: float = 0.7

# Response model
class ChatResponse(BaseModel):
    response: str
    status: str

model = None
tokenizer = None

def load_model():
    global model, tokenizer

    try:
        model_name = "jyanjain/Harshil-karia-Llama-2-7b-chat-finetune"

        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map="auto",
            torch_dtype=torch.float16,
            trust_remote_code=True
        )

        logger.info("Model loaded successfully!")
        return True

    except Exception as e:
        logger.error(f"Error loading model: {str(e)}")
        return False

print("Loading model...")
success = load_model()
if success:
    print("Model loaded successfully!")
else:
    print("Failed to load model")

def generate_response(question: str, max_new_tokens: int = 512, temperature: float = 0.7) -> str:
    try:
        prompt = f"<s>[INST] You are Harshil Karia, founder of Schbang. Answer in a single short paragraph about: {question} [/INST]"

        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                temperature=temperature,
                pad_token_id=tokenizer.eos_token_id,
                use_cache=True
            )

        response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)

        if "[INST]" in response:
            response = response.split("[INST]")[0].strip()
        if "[/INST]" in response:
            response = response.split("[/INST]")[0].strip()

        return response.strip()

    except Exception as e:
        logger.error(f"Error generating response: {str(e)}")
        return "I apologize, but I encountered an error while processing your question. Please try again."


# test_response = generate_response("Who are you?")
# print("ðŸ§ª Test Response:")
# print(test_response)

app = FastAPI(title="Harshil Karia Llama Chat API")

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    try:
        max_new_tokens = min(max(request.max_new_tokens, 50), 512)
        temperature = min(max(request.temperature, 0.1), 1.0)

        logger.info(f"Received message: {request.message}")

        response = generate_response(request.message, max_new_tokens, temperature)

        return ChatResponse(response=response, status="success")

    except Exception as e:
        logger.error(f"Error in chat endpoint: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")


@app.get("/")
async def root():
    return {"message": "Harshil Karia Llama Chat API is running!"}

print("FastAPI app created successfully!")

from pyngrok import ngrok

NGROK_AUTH_TOKEN = "2cur7HzYOxd3WEq1Z6YddReYJAo_6tReATTAeaHBQ89vtMcCw"
ngrok.set_auth_token(NGROK_AUTH_TOKEN)

def setup_ngrok():
    try:
        ngrok.kill()

        public_url = ngrok.connect(8000)
        print(f"Public URL: {public_url}")
        print(f"Chat Endpoint: {public_url}/chat")
        return public_url
    except Exception as e:
        print(f"Error setting up ngrok: {e}")
        return None

def run_server():
    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")

# Setup ngrok
public_url = setup_ngrok()

if public_url:
    server_thread = threading.Thread(target=run_server, daemon=True)
    server_thread.start()

    time.sleep(3)
    print("Server is running!")

    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        print("\nServer stopped!")
        ngrok.kill()
else:
    print("Failed to setup ngrok tunnel")

